{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVIxz9O1N7g6"
   },
   "source": [
    "# Dense Prediction\n",
    "---\n",
    "In this part, you will study a problem of segmentation. The goal of this assignment is to study, implement, and compare different components of dense prediction models, including **data augmentation**, **backbones**, **classifiers** and **losses**.\n",
    "\n",
    "This assignment will require training multiple neural networks, therefore it is advised to use a **GPU** accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI6a6z4eN7hE"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will use a simplified version of a [MasonryWallAnalysis](http://mplab.sztaki.hu/geocomp/masonryWallAnalysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuS-7JPoSD6Q"
   },
   "source": [
    "## Part 1. Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DziabO6AN7hF"
   },
   "source": [
    "### `dataset`\n",
    "**TODO: implement and apply data augmentations**\n",
    "\n",
    "You'll need to study a popular augmentations library: [Albumentations](https://albumentations.ai/), and implement the requested augs. Remember that geometric augmentations need to be applied to both images and masks at the same time, and Albumentations has [native support](https://albumentations.ai/docs/getting_started/mask_augmentation/) for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IfBtHlwB1Lzd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "class UnNormalize:\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "class FloodNet(Dataset):\n",
    "    \"\"\"\n",
    "    Labels semantic:\n",
    "    0: Background, 1: Building, 2: Road, 3: Water, 4: Tree, 5: Vehicle, 6: Pool, 7: Grass\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        phase: str,\n",
    "        augment: bool,\n",
    "        img_size: int,\n",
    "    ):\n",
    "        self.num_classes = 2\n",
    "        self.data_path = data_path\n",
    "        self.phase = phase\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        if phase == 'test':\n",
    "            self.phase = f'{self.phase}/Test Set (1)'\n",
    "\n",
    "        self.items = [\n",
    "            filename.split('.')[0]\n",
    "            for filename in os.listdir(f'{self.data_path}/{self.phase}/images')\n",
    "            if len(filename.split('.')[0]) > 0\n",
    "        ]\n",
    "        \n",
    "        # TODO: implement augmentations (3.5 points)\n",
    "        if augment:\n",
    "            # TODO:\n",
    "            # Random resize\n",
    "            # Random crop (within image borders, output size = img_size)\n",
    "            # Random rotation\n",
    "            # Random horizontal and vertical Flip\n",
    "            # Random color augmentation\n",
    "            self.transform = A.Compose([\n",
    "                A.RandomScale(\n",
    "                    scale_limit=(0.5, 1.5),\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.RandomCrop(\n",
    "                    width=self.img_size,\n",
    "                    height=self.img_size,\n",
    "                    p=1.,\n",
    "                ),\n",
    "                A.RandomRotate90(\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.VerticalFlip(\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.HorizontalFlip(\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.Blur(\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=MEAN,\n",
    "                    std=STD,\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            # TODO: random crop to img_size\n",
    "            self.transform = A.Compose([\n",
    "                A.RandomCrop(\n",
    "                    width=self.img_size,\n",
    "                    height=self.img_size,\n",
    "                    p=1.,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=MEAN,\n",
    "                    std=STD,\n",
    "                ),\n",
    "            ])\n",
    "            \n",
    "        self.base_tf = A.Compose([\n",
    "            A.Resize(\n",
    "                width=self.img_size,\n",
    "                height=self.img_size,\n",
    "                p=1.,\n",
    "            ),\n",
    "            A.Normalize(\n",
    "                mean=MEAN,\n",
    "                std=STD,\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        self.to_tensor = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.asarray(Image.open(f'{self.data_path}/{self.phase}/images/{self.items[index]}.png'))\n",
    "        mask = np.asarray(Image.open(f'{self.data_path}/{self.phase}/labels/{self.items[index]}.png')).clip(0, 1)\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            # TODO: apply transform to both image and mask (0.5 points)\n",
    "\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                mask=mask,\n",
    "            )\n",
    "\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            \n",
    "            transformed = self.base_tf(\n",
    "                image=image,\n",
    "                mask=mask,\n",
    "            )\n",
    "\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "            \n",
    "        image = self.to_tensor(image.copy())\n",
    "        mask = torch.from_numpy(mask.copy()).long()\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            assert isinstance(image, torch.FloatTensor) and image.shape == (3, self.img_size, self.img_size)\n",
    "            assert isinstance(mask, torch.LongTensor) and mask.shape == (self.img_size, self.img_size)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Qcudg1N7hG"
   },
   "source": [
    "### `model`\n",
    "**TODO: Implement the required models.**\n",
    "\n",
    "Typically, all segmentation networks consist of an encoder and decoder. Below is a scheme for a popular DeepLab v3 architecture:\n",
    "\n",
    "<img src=\"https://i.imgur.com/cdlkxvp.png\" />\n",
    "\n",
    "The encoder consists of a convolutional backbone, typically with extensive use of convs with dilations (atrous convs) and a head, which helps to further boost the receptive field. As you can see, the general idea for the encoders is to have as big of a receptive field, as possible.\n",
    "\n",
    "The decoder either does upsampling with convolutions (similarly to the scheme above, or to UNets), or even by simply interpolating the outputs of the encoder.\n",
    "\n",
    "In this assignment, you will need to implement **UNet** and **DeepLab** models. Example UNet looks like this:\n",
    "\n",
    "<img src=\"https://i.imgur.com/uVdcE4e.png\" />\n",
    "\n",
    "For **DeepLab** model we will have three variants for backbones: **ResNet18**, **VGG11 (with BatchNorm)**, and **MobileNet v3 (small).** Use `torchvision.models` to obtain pre-trained versions of these backbones and simply extract their convolutional parts. To familiarize yourself with **MobileNet v3** model, follow this [link](https://paperswithcode.com/paper/searching-for-mobilenetv3).\n",
    "\n",
    "We will also use **Atrous Spatial Pyramid Pooling (ASPP)** head. Its scheme can be seen in the DeepLab v3 architecture above. ASPP is one of the blocks which greatly increases the spatial size of the model, and hence boosts the model's performance. For more details, you can refer to this [link](https://paperswithcode.com/method/aspp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g4YZ3736KaFd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class DoubleBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        middle_channels: int=None,\n",
    "        dropout: bool=False,\n",
    "        p: float=0.2,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if middle_channels is None:\n",
    "            middle_channels = out_channels\n",
    "\n",
    "        self.first_step = nn.Sequential(\n",
    "            nn.Dropout(p) if dropout else nn.Identity(),\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                middle_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=(1, 1),\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.second_step = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                middle_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=(1, 1),\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        x = self.first_step(x)\n",
    "        x = self.second_step(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: list,\n",
    "        max_channels: int,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        last_or_not = lambda i, length: True if i == length - 1 else False\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DoubleBlock(\n",
    "                in_channels=channels[i],\n",
    "                out_channels=channels[i + 1],\n",
    "                middle_channels=max_channels if last_or_not(i, len(channels)) else channels[i + 1]\n",
    "            )\n",
    "            for i in range(len(channels) - 1)\n",
    "        ])\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(\n",
    "            kernel_size=(2, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> tuple:\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for block in self.blocks:\n",
    "\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: list,\n",
    "        p: float=0.2,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DoubleBlock(\n",
    "                in_channels=self.channels[i],\n",
    "                out_channels=self.channels[i + 1],\n",
    "                dropout=True,\n",
    "                p=p,\n",
    "            )\n",
    "            for i in range(len(self.channels) - 1)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.channels[i],\n",
    "                out_channels=self.channels[i + 1],\n",
    "                kernel_size=(2, 2),\n",
    "                stride=(2, 2),\n",
    "            )\n",
    "            for i in range(len(self.channels) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        features: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        for i in range(len(self.channels) - 1):\n",
    "\n",
    "            x = self.upsample[i](x)\n",
    "\n",
    "            _, _, H, W = x.shape\n",
    "            _, _, H_feat, W_feat = features[i].shape\n",
    "            \n",
    "            if H != H_feat or W != W_feat:\n",
    "\n",
    "                features[i] = torch.nn.functional.interpolate(\n",
    "                    input=features[i],\n",
    "                    size=(H, W),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True,\n",
    "                )\n",
    "\n",
    "            x = torch.cat(\n",
    "                tensors=[x, features[i]],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: 8 points\n",
    "\n",
    "    A standard UNet network (with padding in covs).\n",
    "\n",
    "    For reference, see the scheme in materials/unet.png\n",
    "    - Use batch norm between conv and relu\n",
    "    - Use max pooling for downsampling\n",
    "    - Use conv transpose with kernel size = 3, stride = 2, padding = 1, and output padding = 1 for upsampling\n",
    "    - Use 0.5 dropout after concat\n",
    "\n",
    "    Args:\n",
    "      - num_classes: number of output classes\n",
    "      - min_channels: minimum number of channels in conv layers\n",
    "      - max_channels: number of channels in the bottleneck block\n",
    "      - num_down_blocks: number of blocks which end with downsampling\n",
    "\n",
    "    The full architecture includes downsampling blocks, a bottleneck block and upsampling blocks\n",
    "\n",
    "    You also need to account for inputs which size does not divide 2**num_down_blocks:\n",
    "    interpolate them before feeding into the blocks to the nearest size which divides 2**num_down_blocks,\n",
    "    and interpolate output logits back to the original shape\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_classes,\n",
    "        min_channels: int=32,\n",
    "        max_channels: int=512, \n",
    "        num_down_blocks: int=5,\n",
    "        input_channels: int=3,\n",
    "    ) -> None:\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        channels = self.make_channels(\n",
    "            min_channels=min_channels,\n",
    "            num_down_blocks=num_down_blocks,\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            channels=channels[::-1],\n",
    "        )\n",
    "        self.encoder = Encoder(\n",
    "            channels=[input_channels, ] + channels,\n",
    "            max_channels=max_channels,\n",
    "        )\n",
    "        self.head = nn.Conv2d(\n",
    "            in_channels=channels[0],\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=(1, 1),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_channels(\n",
    "        min_channels: int,\n",
    "        num_down_blocks: int,\n",
    "    ) -> list:\n",
    "\n",
    "        output = [\n",
    "            min_channels*(2**i)\n",
    "            for i in range(num_down_blocks)\n",
    "        ]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        features = self.encoder(x)[::-1]\n",
    "\n",
    "        x, last = features[0], features[1:]\n",
    "\n",
    "        out = self.decoder(\n",
    "            x=x,\n",
    "            features=last,\n",
    "        )\n",
    "\n",
    "        logits = self.head(out)\n",
    "        \n",
    "        _, _, H_feat, W_feat = logits.shape\n",
    "            \n",
    "        if H != H_feat or W != W_feat:\n",
    "\n",
    "            logits = torch.nn.functional.interpolate(\n",
    "                input=logits,\n",
    "                size=(H, W),\n",
    "                mode='bilinear',\n",
    "                align_corners=True,\n",
    "            )\n",
    "\n",
    "        assert logits.shape == (B, self.num_classes, H, W), \\\n",
    "            f'Wrong shape of the logits. Got: {logits.shape}, expected: {(B, self.num_classes, H, W)}'\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def init_weights(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight,\n",
    "                    mode='fan_out',\n",
    "                    nonlinearity='relu',\n",
    "                )\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class DeepLab(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: 6 points\n",
    "\n",
    "    (simplified) DeepLab segmentation network.\n",
    "    \n",
    "    Args:\n",
    "      - backbone: ['resnet18', 'vgg11_bn', 'mobilenet_v3_small'],\n",
    "      - aspp: use aspp module\n",
    "      - num classes: num output classes\n",
    "\n",
    "    During forward pass:\n",
    "      - Pass inputs through the backbone to obtain features\n",
    "      - Apply ASPP (if needed)\n",
    "      - Apply head\n",
    "      - Upsample logits back to the shape of the inputs\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,\n",
    "        aspp: bool,\n",
    "        num_classes: int,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(DeepLab, self).__init__()\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.backbone, self.out_features = self.get_features(\n",
    "            backbone_name=backbone,\n",
    "            freeze=True,\n",
    "            pretrained=True,\n",
    "        )\n",
    "\n",
    "        if aspp:\n",
    "            self.aspp = ASPP(self.out_features, 256, [12, 24, 36])\n",
    "        else:\n",
    "            self.aspp = None\n",
    "\n",
    "        self.head = DeepLabHead(self.out_features, num_classes)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_features(\n",
    "        backbone_name: str,\n",
    "        freeze: bool=False,\n",
    "        pretrained: bool=False,\n",
    "    ) -> list:\n",
    "        \n",
    "        backbone = getattr(models, backbone_name)(\n",
    "            pretrained=pretrained,\n",
    "        )\n",
    "        \n",
    "        for param in backbone.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "            \n",
    "        blocks = list(backbone.children())\n",
    "        \n",
    "        convs, avg, clf = blocks[:-2], blocks[-2], blocks[-1]\n",
    "        \n",
    "        if isinstance(clf, nn.Sequential):\n",
    "            clf = clf[0]\n",
    "        \n",
    "        features = torch.nn.Sequential(*convs)\n",
    "        \n",
    "        if isinstance(avg.output_size, (tuple, list)):\n",
    "            devider = avg.output_size[0] * avg.output_size[1]\n",
    "        else:\n",
    "            devider = avg.output_size * avg.output_size\n",
    "            \n",
    "        output_features = clf.in_features // devider\n",
    "        \n",
    "        return features, output_features\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if self.aspp is not None:\n",
    "            features = self.aspp(features)\n",
    "            \n",
    "        logits = self.head(features)\n",
    "        \n",
    "        _, _, H_out, W_out = logits.shape\n",
    "        \n",
    "        if H != H_out or W != W_out:\n",
    "\n",
    "            logits = torch.nn.functional.interpolate(\n",
    "                input=logits,\n",
    "                size=(H, W),\n",
    "                mode='bilinear',\n",
    "                align_corners=True,\n",
    "            )\n",
    "\n",
    "        assert logits.shape == (B, self.num_classes, H, W), 'Wrong shape of the logits'\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DeepLabHead(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        \n",
    "        super(DeepLabHead, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, num_classes, 1),\n",
    "        )\n",
    "\n",
    "class ASPPBlock(nn.Sequential):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        dilation: int,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(ASPPBlock, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: 8 points\n",
    "\n",
    "    Atrous Spatial Pyramid Pooling module\n",
    "    with given atrous_rates and out_channels for each head\n",
    "    Description: https://paperswithcode.com/method/aspp\n",
    "    \n",
    "    Detailed scheme: materials/deeplabv3.png\n",
    "      - \"Rates\" are defined by atrous_rates\n",
    "      - \"Conv\" denotes a Conv-BN-ReLU block\n",
    "      - \"Image pooling\" denotes a global average pooling, followed by a 1x1 \"conv\" block and bilinear upsampling\n",
    "      - The last layer of ASPP block should be Dropout with p = 0.5\n",
    "\n",
    "    Args:\n",
    "      - in_channels: number of input and output channels\n",
    "      - num_channels: number of output channels in each intermediate \"conv\" block\n",
    "      - atrous_rates: a list with dilation values\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_channels, atrous_rates):\n",
    "        \n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            ASPPBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=atrous_rate,\n",
    "                dilation=atrous_rate,\n",
    "            )\n",
    "            for atrous_rate in atrous_rates\n",
    "        ])\n",
    "        \n",
    "        self.pooling = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(in_channels, num_channels, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(num_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels*4, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.PReLU(in_channels),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # TODO: forward pass through the ASPP module\n",
    "        \n",
    "        features = [\n",
    "            block(x)\n",
    "            for block in self.blocks\n",
    "        ]\n",
    "        \n",
    "        features.append(\n",
    "            torch.nn.functional.interpolate(\n",
    "                input=self.pooling(x),\n",
    "                size=features[-1].shape[2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        out = torch.cat(\n",
    "            tensors=features,\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        res = self.conv1x1(out)\n",
    "            \n",
    "        assert res.shape[1] == x.shape[1], 'Wrong number of output channels'\n",
    "        assert res.shape[2] == x.shape[2] and res.shape[3] == x.shape[3], 'Wrong spatial size'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VDn5sE3N7hH"
   },
   "source": [
    "### `loss`\n",
    "**TODO: implement test losses.**\n",
    "\n",
    "For validation, we will use three metrics. \n",
    "- Mean intersection over union: **mIoU**,\n",
    "- Mean class accuracy: **classAcc**,\n",
    "- Accuracy: **Acc**.\n",
    "\n",
    "To calculate **IoU**, use this formula for binary segmentation masks for each class, and then average w.r.t. all classes:\n",
    "\n",
    "$$ \\text{IoU} = \\frac{ \\text{area of intersection} }{ \\text{area of union} } = \\frac{ \\| \\hat{m} \\cap m  \\| }{ \\| \\hat{m} \\cup m \\| }, \\quad \\text{$\\hat{m}$ — predicted binary mask},\\ \\text{$m$ — target binary mask}.$$\n",
    "\n",
    "For **mRecall** you can use the following formula:\n",
    "\n",
    "$$\n",
    "    \\text{mRecall} = \\frac{ \\| \\hat{m} \\cap m \\| }{ \\| m \\| }\n",
    "$$\n",
    "\n",
    "And **accuracy** is a fraction of correctly identified pixels in the image.\n",
    "\n",
    "Generally, we want our models to optimize accuracy since this implies that it makes little mistakes. However, most of the segmentation problems have imbalanced classes, and therefore the models tend to underfit the rare classes. Therefore, we also need to measure the mean performance of the model across all classes (mean IoU or mean class accuracy). In reality, these metrics (not the accuracy) are the go-to benchmarks for segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Ekvzad5Ke39"
   },
   "outputs": [],
   "source": [
    "def calc_val_data(preds, masks, num_classes):\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "    \n",
    "    intersection = torch.stack(\n",
    "        tensors=[\n",
    "            ((masks == i) & (preds == i)).sum((1, 2))\n",
    "            for i in range(num_classes)\n",
    "        ],\n",
    "        dim=-1,\n",
    "    ) #calc intersection for each class\n",
    "    union = torch.stack(\n",
    "        tensors=[\n",
    "            ((masks == i) | (preds == i)).sum((1, 2))\n",
    "            for i in range(num_classes)\n",
    "        ],\n",
    "        dim=-1,\n",
    "    ) #calc union for each class\n",
    "\n",
    "    target = torch.stack(\n",
    "        tensors=[\n",
    "            (masks == i).sum((1, 2))\n",
    "            for i in range(num_classes)\n",
    "        ],\n",
    "        dim=-1,\n",
    "    ) #calc number of pixels in groundtruth mask per class\n",
    "    # Output shapes: B x num_classes\n",
    "\n",
    "    assert isinstance(intersection, torch.Tensor), 'Output should be a tensor'\n",
    "    assert isinstance(union, torch.Tensor), 'Output should be a tensor'\n",
    "    assert isinstance(target, torch.Tensor), 'Output should be a tensor'\n",
    "\n",
    "    assert intersection.shape == union.shape == target.shape, 'Wrong output shape'\n",
    "    assert union.shape[0] == masks.shape[0] and union.shape[1] == num_classes, 'Wrong output shape'\n",
    "\n",
    "    return intersection, union, target\n",
    "\n",
    "def calc_val_loss(intersection, union, target, eps = 1e-7):\n",
    "    \n",
    "    iou = torch.nanmean(\n",
    "        intersection / union,\n",
    "        dim=1,\n",
    "    )\n",
    "    rec = torch.nanmean(\n",
    "        intersection / target,\n",
    "        dim=1,\n",
    "    )\n",
    "    acc = 1 - (union - intersection).sum(dim=1) / (2*target.sum(dim=1))\n",
    "    \n",
    "    mean_iou = iou.mean() # TODO: calc mean class iou\n",
    "    mean_class_rec = rec.mean() # TODO: calc mean class recall\n",
    "    mean_acc = acc.mean() # TODO: calc mean accuracy\n",
    "\n",
    "    return mean_iou, mean_class_rec, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRiIQ1_5N7hH"
   },
   "source": [
    "### `train`\n",
    "**TODO: define optimizer and learning rate scheduler.**\n",
    "\n",
    "You need to experiment with different optimizers and schedulers and pick one of each which works the best. Since the grading will be partially based on the validation performance of your models, we strongly advise doing some preliminary experiments and pick the configuration with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ur5xGQ_wKzGp"
   },
   "outputs": [],
   "source": [
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Modifications Copyright Skoltech Deep Learning Course.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        backbone: str,\n",
    "        aspp: bool,\n",
    "        augment_data: bool,\n",
    "        optimizer: str = 'default',\n",
    "        scheduler: str = 'default',\n",
    "        lr: float = None,\n",
    "        batch_size: int = 16,\n",
    "        data_path: str = '../data/PPKE-SZTAKI-MasonryBenchmark',\n",
    "        image_size: int = 512,\n",
    "        num_classes: int = 2,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(SegModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if model == 'unet':\n",
    "            self.net = UNet(num_classes=self.num_classes)\n",
    "        elif model == 'deeplab':\n",
    "            self.net = DeepLab(backbone, aspp, self.num_classes)\n",
    "\n",
    "        self.train_dataset = FloodNet(data_path, 'train', augment_data, image_size)\n",
    "        self.test_dataset = FloodNet(data_path, 'test', augment_data, image_size)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.lr = lr\n",
    "        self.eps = 1e-7\n",
    "\n",
    "        self.unnorm = UnNormalize(\n",
    "            mean=MEAN,\n",
    "            std=STD,\n",
    "        )\n",
    "\n",
    "        # Visualization\n",
    "        self.color_map = torch.FloatTensor(\n",
    "            [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "             [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.forward(img)\n",
    "\n",
    "        train_loss = F.cross_entropy(pred, mask)\n",
    "\n",
    "        self.log('train_loss', train_loss, prog_bar=True)\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        pred = self.forward(img)\n",
    "        \n",
    "        val_loss = F.cross_entropy(pred, mask)\n",
    "\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "\n",
    "        intersection, union, target = calc_val_data(pred, mask, self.num_classes)\n",
    "\n",
    "        return {'intersection': intersection, 'union': union, 'target': target, 'img': self.unnorm(img), 'pred': pred, 'mask': mask}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        intersection = torch.cat([x['intersection'] for x in outputs])\n",
    "        union = torch.cat([x['union'] for x in outputs])\n",
    "        target = torch.cat([x['target'] for x in outputs])\n",
    "\n",
    "        mean_iou, mean_class_rec, mean_acc = calc_val_loss(intersection, union, target, self.eps)\n",
    "\n",
    "        log_dict = {'mean_iou': mean_iou, 'mean_class_rec': mean_class_rec, 'mean_acc': mean_acc}\n",
    "\n",
    "        for k, v in log_dict.items():\n",
    "            self.log(k, v, prog_bar=True)\n",
    "\n",
    "        # Visualize results\n",
    "        img = torch.cat([x['img'] for x in outputs]).cpu()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu()\n",
    "        mask = torch.cat([x['mask'] for x in outputs]).cpu()\n",
    "\n",
    "        pred_vis = self.visualize_mask(torch.argmax(pred, dim=1))\n",
    "        mask_vis = self.visualize_mask(mask)\n",
    "\n",
    "        results = torch.cat(torch.cat([img, pred_vis, mask_vis], dim=3).split(1, dim=0), dim=2)\n",
    "        results_thumbnail = F.interpolate(results, scale_factor=0.25, mode='bilinear', align_corners=True)[0]\n",
    "\n",
    "        self.logger.experiment.add_image('results', results_thumbnail, self.current_epoch)\n",
    "\n",
    "    def visualize_mask(self, mask):\n",
    "        b, h, w = mask.shape\n",
    "        mask_ = mask.view(-1)\n",
    "\n",
    "        if self.color_map.device != mask.device:\n",
    "            self.color_map = self.color_map.to(mask.device)\n",
    "\n",
    "        mask_vis = self.color_map[mask_].view(b, h, w, 3).permute(0, 3, 1, 2).clone()\n",
    "\n",
    "        return mask_vis\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO: 2 points\n",
    "        # Use self.optimizer and self.scheduler to call different optimizers\n",
    "        opt = torch.optim.Adam(self.net.parameters(), lr=self.lr) # TODO: init optimizer\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, step_size=20) # TODO: init learning rate scheduler\n",
    "        return [opt], [sch]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, num_workers=8, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, num_workers=8, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi3TDmYyN7hI"
   },
   "source": [
    "## Part 2. Train and benchmark\n",
    "\n",
    "In this part of the assignment, you need to train the following models and measure their training time:\n",
    "- **UNet** (with and without data augmentation),\n",
    "- **DeepLab** with **ResNet18** backbone (with **ASPP** = True and False),\n",
    "- **DeepLab** with the remaining backbones you implemented and **ASPP** = True).\n",
    "\n",
    "To get the full mark for this assignment, all the required models should be trained (and their checkpoints provided), and have at least 0.5 accuracies.\n",
    "\n",
    "After the models are trained, evaluate their inference time on both GPU and CPU.\n",
    "\n",
    "Example training and evaluation code are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dFKHCO_DN7hJ"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def define_model(\n",
    "    model_name: str, \n",
    "    backbone: str, \n",
    "    aspp: bool, \n",
    "    augment_data: bool, \n",
    "    optimizer: str, \n",
    "    scheduler: str, \n",
    "    lr: float, \n",
    "    checkpoint_name: str = '', \n",
    "    batch_size: int = 8,\n",
    "    num_classes: int = 2,\n",
    "):\n",
    "    assignment_dir = 'semantic_segmentation'\n",
    "    experiment_name = f'{model_name}_{backbone}_augment={augment_data}_aspp={aspp}'\n",
    "    model_name = model_name.lower()\n",
    "    backbone = backbone.lower() if backbone is not None else backbone\n",
    "    \n",
    "    model = SegModel(\n",
    "        model_name, \n",
    "        backbone, \n",
    "        aspp, \n",
    "        augment_data,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        lr,\n",
    "        batch_size,\n",
    "        data_path='../data/PPKE-SZTAKI-MasonryBenchmark',\n",
    "        image_size=512,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    if checkpoint_name:\n",
    "        model.load_state_dict(torch.load(f'{assignment_dir}/logs/{experiment_name}/{checkpoint_name}')['state_dict'])\n",
    "    \n",
    "    return model, experiment_name\n",
    "\n",
    "def train(model, experiment_name, use_gpu):\n",
    "    assignment_dir = 'semantic_segmentation'\n",
    "\n",
    "    logger = pl.loggers.TensorBoardLogger(save_dir=f'{assignment_dir}/logs', name=experiment_name)\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor='mean_iou',\n",
    "        dirpath=f'{assignment_dir}/logs/{experiment_name}',\n",
    "        filename='{epoch:02d}-{mean_iou:.3f}',\n",
    "        mode='max',\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100, \n",
    "        gpus=1 if use_gpu else None, \n",
    "        benchmark=True, \n",
    "        check_val_every_n_epoch=5, \n",
    "        logger=logger, \n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    trainer.fit(model)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    time_end = time.time()\n",
    "    \n",
    "    training_time = (time_end - time_start) / 60\n",
    "    \n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | UNet | 7.8 M \n",
      "------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.052    Total estimated model params size (MB)\n",
      "/home/ilyabasharov/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/ilyabasharov/workspace/masonry_detection/notebooks/semantic_segmentation/logs/Unet_None_augment=True_aspp=None exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyabasharov/.local/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "/home/ilyabasharov/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ff73c3ca244cf190801a1f431acb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 24.335324863592785\n"
     ]
    }
   ],
   "source": [
    "model, experiment_name = define_model(\n",
    "    model_name='Unet',\n",
    "    backbone=None,\n",
    "    aspp=None,\n",
    "    augment_data=True,\n",
    "    optimizer='adam', # use these options to experiment\n",
    "    scheduler='step_lr', # with optimizers and schedulers\n",
    "    lr=0.01, # experiment to find the best LR\n",
    "    num_classes=2,\n",
    ")\n",
    "\n",
    "training_time = train(model, experiment_name, use_gpu=True)\n",
    "print('Training time:', training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhZ7Lxl1N7hK"
   },
   "source": [
    "After training, the loss curves and validation images with their segmentation masks can be viewed using the TensorBoard extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hk5ZiCKZN7hL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-31599ab1555fec29\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-31599ab1555fec29\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir semantic_segmentation/logs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2-semantic_segmentation-Surname-Name-attempt-1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
